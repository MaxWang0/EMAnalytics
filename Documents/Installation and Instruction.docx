Java installation in google server:

sudo apt-get update
java -version
sudo apt-get install default-jre
sudo apt-get install default-jdk
sudo apt-get install openjdk-7-jre 
sudo apt-get install openjdk-7-jdk


Apache2 installation in google server:
sudo apt-get update
sudo apt-get install apache2

Nodejs4.0 installation in google server
http://linoxide.com/ubuntu-how-to/setup-node-js-4-0-ubuntu-14-04-15-04/
apt-get update
apt-get install python gcc make g++ wget
wget https://nodejs.org/download/rc/v4.0.0-rc.1/node-v4.0.0-rc.1.tar.gz
tar –zxvf node-v4.0.0-rc.1.tar.gz
root
tar –zxvf node-v4.0.0-rc.1.tar.gz
./configure
Make install

Mongodb3.2 installation in google server

sudo apt-key adv –keyserver hkp://keyserver.ubuntu.com:80 –recv EA312927
echo “deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse” | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list
sudo apt-get update
sudo apt-get install –y mongodb-org=3.2.1 mongodb-org-server=3.2.1 mongodb-org-shell=3.2.1 mongodb-org-mongos=3.2.1 mongodb-org-tools=3.2.1



scala installation:

$wget http://www.scala-lang.org/files/archive/scala-2.10.4.tgz
$sudo mkdir /usr/local/src/scala
$sudo tar –xvf scala-2.10.4.tgz –C /usr/local/src/scala/

$vi .bashrc
Export SCALA_HOME=/usr/local/src/scala/scala-2.10.4
Export PATH=$SCALA_HOME/bin:$PATH

$. .bashrc
Spark 1.5.0 installation in google server

wget   http://d3kbcqa49mib13.cloudfront.net/spark1.5.0-bin-hadoop2.6.tgz                                                     
tar –xvf spark-1.5.0-bin-hadoop2.6.tgz

R installation

Sudo apt-get update
Sudo apt-get –f install
Sudo apt-get –y install r-base

We need to use spark1.5.0-prebuilt vertion to run the spark-mongodb package
first of all enter the sparkR shell

./bin/sparkR --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1

run the following code:

sc <- sparkR.init(appName="SparkR-DataFrame-example")
Sys.setenv('SPARKR_SUBMIT_ARGS'='"--packages" "com.databricks:spark-csv_2.10:1.3.0" "sparkr-shell"')
sqlContext <- sparkRSQL.init(sc)

filename <- unzip("cdr.00011485.0000000000000000.20160119.095517854.LMPSELECTBUSNP6787_20160119_095512_csv.zip", , list = TRUE)
filename <- as.character(filename[1])
Zip1_df <- read.csv(unz("cdr.00011485.0000000000000000.20160119.095517854.LMPSELECTBUSNP6787_20160119_095512_csv.zip", filename), header = T, sep = ",")
Zip1_DF <- createDataFrame(sqlContext, Zip1_df)
registerTempTable(Zip1_DF, "ZIP1")
newzip1 <- sql(sqlContext, "SELECT * FROM ZIP1 WHERE ElectricalBus LIKE 'MOBILE%'")
#display the average LMP
head(avg(groupBy(newzip1)))
#write to the parquet file
write.df(newzip1, "selectedBus.parquet", "parquet", "overwrite")

enter the spark-shell
./bin/spark-shell --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1

run the following code:

import org.apache.spark.sql.SQLContext._
import org.apache.spark.sql._
import sqlContext._
import com.mongodb.casbah.{WriteConcern => MongodbWriteConcern}
import com.stratio.datasource.mongodb._
import MongodbConfig._

#create and write parquet file back to mongodb
val parquetFile = sqlContext.read.parquet("students.parquet")
val options = Map("host" -> "localhost:27017", "database" -> "highschool", "collection" -> "students")
parquetFile.write.format("com.stratio.datasource.mongodb").mode(SaveMode.Append).options(options).save()

#read csv file and save back to mongodb
val ercotdata = sc.textFile("/home/emotto_user2/testdata/cdr.00011485.0000000000000000.20160115.112014.LMPSELECTBUSNP6787_20160115_112011.csv")
case class X(SCEDTimestamp: String, RepeatedHourFlag: String, ElectricalBus: String, LMP: Double)
val header = ercotdata.first()
val data = ercotdata.filter(x => x != header)
val dataframe = data.map(_.split(",")).map(p => X(p(0).toString, p(1).toString, p(2).toString, p(3).toDouble)).toDF()
val saveConfig = MongodbConfigBuilder(Map(Host -> List("localhost:27017"), Database -> "Ercot1", Collection -> "BUS1", SamplingRatio -> 1.0, WriteConcern -> MongodbWriteConcern.Normal, SplitSize -> 8, SplitKey -> "_id"))
dataframe.saveToMongodb(saveConfig.build)

#createtable.scala
update and insert newtable to accumulative table, need to be run in the mongo-spark package


